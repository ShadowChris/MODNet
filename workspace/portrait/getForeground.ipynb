{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from src.models.modnet import MODNet\n",
    "# python -m demo.image_matting.colab.inference --input-path data/input --output-path data/mask --ckpt-path pretrained/modnet_photographic_portrait_matting.ckpt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'E:\\\\data\\\\workspace\\\\python\\\\wanxiang-ai\\\\MODNet'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = \"data/input\"\n",
    "# mask_path = \"data/mask\"\n",
    "output_path = \"data/output\"\n",
    "# ckpt_path = \"pretrained/modnet_photographic_portrait_matting.ckpt\"\n",
    "ckpt_path = \"pretrained/modnet_photographic_portrait_matting.ckpt\"\n",
    "\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process image: 1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\modnet\\lib\\site-packages\\torch\\nn\\functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    }
   ],
   "source": [
    "# check input arguments\n",
    "if not os.path.exists(input_path):\n",
    "    print('Cannot find input path: {0}'.format(input_path))\n",
    "    exit()\n",
    "if not os.path.exists(output_path):\n",
    "    print('Cannot find mask path: {0}'.format(output_path))\n",
    "    exit()\n",
    "if not os.path.exists(ckpt_path):\n",
    "    print('Cannot find ckpt path: {0}'.format(ckpt_path))\n",
    "    exit()\n",
    "\n",
    "# define hyper-parameters\n",
    "ref_size = 512\n",
    "\n",
    "# define image to tensor transform\n",
    "im_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create MODNet and load the pre-trained ckpt\n",
    "modnet = MODNet(backbone_pretrained=False)\n",
    "modnet = nn.DataParallel(modnet)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    modnet = modnet.cuda()\n",
    "    weights = torch.load(ckpt_path)\n",
    "else:\n",
    "    weights = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "modnet.load_state_dict(weights)\n",
    "modnet.eval()\n",
    "\n",
    "# inference images\n",
    "im_names = os.listdir(input_path)\n",
    "for im_name in im_names:\n",
    "    print('Process image: {0}'.format(im_name))\n",
    "\n",
    "    # read image\n",
    "    im = Image.open(os.path.join(input_path, im_name))\n",
    "\n",
    "    # unify image channels to 3\n",
    "    im = np.asarray(im)\n",
    "    if len(im.shape) == 2:\n",
    "        im = im[:, :, None]\n",
    "    if im.shape[2] == 1:\n",
    "        im = np.repeat(im, 3, axis=2)\n",
    "    elif im.shape[2] == 4:\n",
    "        im = im[:, :, 0:3]\n",
    "\n",
    "    # convert image to PyTorch tensor\n",
    "    im = Image.fromarray(im)\n",
    "    im_tensor = im_transform(im)\n",
    "\n",
    "    # add mini-batch dim\n",
    "    im_tensor = im_tensor[None, :, :, :]\n",
    "\n",
    "    # resize image for input\n",
    "    im_b, im_c, im_h, im_w = im_tensor.shape\n",
    "    if max(im_h, im_w) < ref_size or min(im_h, im_w) > ref_size:\n",
    "        if im_w >= im_h:\n",
    "            im_rh = ref_size\n",
    "            im_rw = int(im_w / im_h * ref_size)\n",
    "        elif im_w < im_h:\n",
    "            im_rw = ref_size\n",
    "            im_rh = int(im_h / im_w * ref_size)\n",
    "    else:\n",
    "        im_rh = im_h\n",
    "        im_rw = im_w\n",
    "\n",
    "    im_rw = im_rw - im_rw % 32\n",
    "    im_rh = im_rh - im_rh % 32\n",
    "    im_tensor_resized = F.interpolate(im_tensor, size=(im_rh, im_rw), mode='area', recompute_scale_factor=False)\n",
    "\n",
    "    # inference\n",
    "    _, _, matte = modnet(im_tensor_resized.cuda() if torch.cuda.is_available() else im_tensor_resized, True)\n",
    "\n",
    "    # resize and save matte\n",
    "    matte = F.interpolate(matte, size=(im_h, im_w), mode='area', recompute_scale_factor=False)\n",
    "    matte_np = matte[0][0].data.cpu().numpy()\n",
    "\n",
    "     # convert matte to range [0, 1]\n",
    "    matte_np = np.expand_dims(matte_np, axis=2)\n",
    "    # extract foreground by multiplying original image with matte\n",
    "    foreground = im * matte_np\n",
    "\n",
    "\n",
    "    ## 1. 背景黑色\n",
    "    # convert foreground to PIL Image and save\n",
    "    # foreground = foreground.astype(np.uint8)\n",
    "    # foreground_img = Image.fromarray(foreground)\n",
    "    # foreground_name = im_name.split('.')[0] + '_foreground.png'\n",
    "    # foreground_img.save(os.path.join(output_path, foreground_name))\n",
    "    #--------------------------------------------------\n",
    "    # 2. 生成mask\n",
    "    # matte_name = im_name.split('.')[0] + '.png'\n",
    "    # Image.fromarray(((matte * 255).astype('uint8')), mode='L').save(os.path.join(output_path, matte_name))\n",
    "    #--------------------------------------------------\n",
    "\n",
    "    # 3. 背景透明\n",
    "    # create an empty 4-channel image (RGBA) with the same size as the input image\n",
    "    transparent_foreground = np.zeros((im_h, im_w, 4), dtype=np.uint8)\n",
    "    # set the RGB channels of the transparent_foreground to the extracted foreground\n",
    "    transparent_foreground[:, :, :3] = foreground\n",
    "    # set the alpha channel of the transparent_foreground to the matte (scaled to [0, 255])\n",
    "    transparent_foreground[:, :, 3] = (matte_np * 255).astype(np.uint8).squeeze()\n",
    "    # convert transparent_foreground to a PIL Image and save\n",
    "    transparent_foreground_img = Image.fromarray(transparent_foreground, mode='RGBA')\n",
    "    transparent_foreground_name = im_name.split('.')[0] + '_transparent_foreground.png'\n",
    "    transparent_foreground_img.save(os.path.join(output_path, transparent_foreground_name))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-39eaf666",
   "language": "python",
   "display_name": "PyCharm (MODNet)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}